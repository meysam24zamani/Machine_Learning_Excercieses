install.packages("Rcmdr")
install.packages("DAAG")
library("Rcmdr", lib.loc="~/R/win-library/3.4")
library("RcmdrMisc", lib.loc="~/R/win-library/3.4")
detach("package:Rcmdr", unload=TRUE)
detach("package:RcmdrMisc", unload=TRUE)
library("Rcmdr", lib.loc="~/R/win-library/3.4")
detach("package:Rcmdr", unload=TRUE)
install.packages("rlang")
library("Rcmdr", lib.loc="~/R/win-library/3.4")
detach("package:Rcmdr", unload=TRUE)
library("Rcmdr", lib.loc="~/R/win-library/3.4")
detach("package:Rcmdr", unload=TRUE)
library("Rcmdr", lib.loc="~/R/win-library/3.4")
install.packages("rlang")
library("Rcmdr", lib.loc="~/R/win-library/3.4")
library("Rcmdr", lib.loc="~/R/win-library/3.4")
library("RcmdrMisc", lib.loc="~/R/win-library/3.4")
library("Hmisc", lib.loc="~/R/win-library/3.4")
library("Rcmdr", lib.loc="~/R/win-library/3.4")
library("RcmdrMisc", lib.loc="~/R/win-library/3.4")
library("Rcmdr", lib.loc="~/R/win-library/3.4")
library("RcmdrMisc", lib.loc="~/R/win-library/3.4")
install.packages("rlang")
library("Rcmdr", lib.loc="~/R/win-library/3.4")
library("RcmdrMisc", lib.loc="~/R/win-library/3.4")
install.packages("rlang")
library("rlang", lib.loc="~/R/win-library/3.4")
library("Rcmdr", lib.loc="~/R/win-library/3.4")
library("car", lib.loc="~/R/win-library/3.4")
library("Rcmdr", lib.loc="~/R/win-library/3.4")
library("Rcmdr", lib.loc="~/R/win-library/3.4")
library("Rcmdr", lib.loc="~/R/win-library/3.4")
?rnorm
help.search(rnorm)
help.search("rnorm")
?rnorm
?rnorm
rnorm
set.seed(2222)
par(mfrow=c(1, 1))
N <- 10
(X <- matrix(c(rep(1,N), seq(N)),nrow=N))
(t <- seq(10,20,length.out=N) + rnorm(N))
plot(X[,2],t,lwd=3)
(C <- t(X) %*% X)                   # X^T X
(X.pseudo <- solve(C) %*% t(X))       # (X^T X)^{-1} X^T
## this should be the identity matrix (thus we obtain a left pseudo-inverse of X)
X.pseudo %*% X
## this is the solution (the coefficient vector)
(w <- X.pseudo %*% t)
# so this is our model ...
lines (X[,2], w[2,1]*X[,2]+w[1,1], type="l")
(s <- svd(X))
D <- diag(s$d)
s$u %*% D %*% t(s$v) # this should be equal to X
D <- diag(1/s$d)
(w.svd <- s$v %*% D %*% t(s$u) %*% t)
# w.svd should be equal to w
all.equal(w,w.svd)
(sample <- data.frame(x=X,t=t))
# 1. turn this off (the "-1" in the formula below) and use our own column of 1's
model1 <- glm (t ~ x.2 + x.1 - 1, data=sample, family = gaussian)
# 2. use this built-in feature and ignore our own column of 1's (recommended)
model2 <- glm (t ~ x.2, data=sample, family = gaussian)
View(model1)
View(model2)
# your coefficients (the w vector)
model1$coefficients
model2$coefficients
eps <- 1e-3
(X.eps <- matrix(c(1,eps,0,1,0,eps),nrow=3))
(C.eps <- t(X.eps) %*% X.eps)
# this is going to break down ...
eps <- 1e-10
(X.eps <- matrix(c(1,eps,0,1,0,eps),nrow=3))
(C.eps <- t(X.eps) %*% X.eps)
solve(C.eps)
X
kappa(X, exact=TRUE)
kappa(t(X) %*% X, exact=TRUE)
## an innocent-looking matrix
(A <- matrix(c(rep(1,N), 100+seq(N)),nrow=N))
kappa(A, exact=TRUE)
kappa(t(A) %*% A, exact=TRUE)
A[,2] <- A[,2] - mean(A[,2])
A
kappa(A, exact=TRUE)
kappa(t(A) %*% A, exact=TRUE)
library(MASS) # ginv()
ginv(A)
set.seed (7)
N <- 20
N.test <- 1000
a <- 0
b <- 1
sigma.square <- 0.3^2
x <- seq(a,b,length.out=N)
t <- sin(2*pi*x) + rnorm(N, mean=0, sd=sqrt(sigma.square))
sample <- data.frame(x=x,t=t)
plot(x,t, lwd=3, ylim = c(-1.1, 1.1))
curve (sin(2*pi*x), a, b, add=TRUE, ylim = c(-1.1, 1.1), col="blue")
abline(0,0, lty=2)
# we begin with polynomials of order 1
model <- glm(t ~ x, data = sample, family = gaussian)
prediction <- predict(model)
abline(model, col="red")
( mean.square.error <- sum((t - prediction)^2)/N )
# alternatively, glm() delivers the deviance = sum of square errors
( mean.square.error <- model$deviance/N )
# we prefer to convert it to normalized root MSE or NRMSE
(norm.root.mse <- sqrt(model$deviance/((N-1)*var(t))))
model <- glm(t ~ poly(x, 2, raw=TRUE), data = sample, family = gaussian)
summary(model)
model$coefficients
# let's plot it in red
plot(x,t, lwd=3, ylim = c(-1.1, 1.1))
curve (sin(2*pi*x), a, b, add=TRUE, ylim = c(-1.1, 1.1), col="blue")
abline(0,0, lty=2)
points(x, predict(model), type="l", col="red", lwd=2)
# get the training normalized root MSE (note it is a bit smaller, as reasonably expected)
( norm.root.mse <- sqrt(model$deviance/((N-1)*var(t))) )
## Let's create now a large test sample, for future use
x.test <- seq(a,b,length.out=N.test)
t.test <- sin(2*pi*x.test) + rnorm(N.test, mean=0, sd=sqrt(sigma.square))
test.sample <- data.frame(x=x.test,t=t.test)
plot(test.sample$x, test.sample$t)
p <- 1
q <- N-1
coef <- model <- vector("list",q-p+1)
norm.root.mse.train <- norm.root.mse.test <- rep(0,q-p+1)
for (i in p:q)
{
model[[i]] <- glm(t ~ poly(x, i, raw=TRUE), data = sample, family = gaussian)
# store the model coefficients, as well as training and test errors
coef[[i]] <- model[[i]]$coefficients
norm.root.mse.train[i] <- sqrt(model[[i]]$deviance/N)
predictions <- predict (model[[i]], newdata=test.sample)
norm.root.mse.test[i] <- sqrt(sum((test.sample$t - predictions)^2)/((N.test-1)*var(test.sample$t)))
}
results <- cbind (Degree=p:q, Coefficients=coef, NRMSE.train=norm.root.mse.train, NRMSE.test=norm.root.mse.test)
plot(results[,1],results[,1], ylim = c(0, 1.1), col="white", xlab="Degree",ylab="errors")
axis(1, at=p:q)
points(x=results[,1],y=results[,3], type="l", col="red", lwd=2)
points(x=results[,1],y=results[,4], type="l", col="blue", lwd=2, add=TRUE)
legend ("topleft", legend=c("TRAINING ERROR","TEST ERROR"),
lty=c(1,1), # gives the legend appropriate symbols (lines)
lwd=c(2.5,2.5), col=c("red","blue")) # gives the legend lines the correct color and width
abline (v=3, lty=2)
coefs.table <- matrix (nrow=10, ncol=9)
for (i in 1:10)
for (j in 1:9)
coefs.table[i,j] <- coef[[j]][i]
coefs.table
pcancer <- read.table("prostate.data", header=TRUE)
summary(pcancer)
# Scale data and prepare train/test split
pcancer.std <- data.frame(cbind(scale(pcancer[,1:8]),lpsa=pcancer$lpsa))
train <- pcancer.std[pcancer$train,]
test <- pcancer.std[!pcancer$train,]
setwd("D:/Spain-Barcelona/UPC/third Semester/ML/Lab4/L04 - linreg")
pcancer <- read.table("prostate.data", header=TRUE)
summary(pcancer)
# Scale data and prepare train/test split
pcancer.std <- data.frame(cbind(scale(pcancer[,1:8]),lpsa=pcancer$lpsa))
train <- pcancer.std[pcancer$train,]
test <- pcancer.std[!pcancer$train,]
dim(train)
dim(test)
plot(train)
round(cor(train),2)
N <- nrow(train)
(model.linreg <- lm(lpsa ~ ., data=train))
# final model and its coeficients (betas):
(model.linreg.FINAL <- step(model.linreg))
beta.linreg.FINAL <- coef(model.linreg.FINAL)
library(MASS)
model.ridge <- lm.ridge(lpsa ~ ., data=train, lambda = seq(0,10,0.1))
plot(seq(0,10,0.1), model.ridge$GCV, main="GCV of Ridge Regression", type="l",
xlab=expression(lambda), ylab="GCV")
# The optimal lambda is given by
(lambda.ridge <- seq(0,10,0.1)[which.min(model.ridge$GCV)])
abline (v=lambda.ridge,lty=2)
# We can plot the coefficients and see how they vary as a function of lambda:
colors <- rainbow(8)
matplot (seq(0,10,0.1), coef(model.ridge)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline (v=lambda.ridge, lty=2)
matplot (seq(0,10,0.1), coef(model.ridge)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline (v=lambda.ridge, lty=2)
matplot (seq(0,10,0.1), coef(model.ridge)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline (v=lambda.ridge, lty=1)
matplot (seq(0,10,0.1), coef(model.ridge)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline (v=lambda.ridge, lty=3)
matplot (seq(0,10,0.1), coef(model.ridge)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline (v=lambda.ridge, lty=4)
matplot (seq(0,10,0.1), coef(model.ridge)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline (v=lambda.ridge, lty=5)
matplot (seq(0,10,0.1), coef(model.ridge)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline (v=lambda.ridge, lty=6)
matplot (seq(0,10,0.1), coef(model.ridge)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline (v=lambda.ridge, lty=7)
matplot (seq(0,10,0.1), coef(model.ridge)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline (v=lambda.ridge, lty=2)
abline (h=0, lty=2)
arrows (5.5,0.45,5,0.35, length = 0.15)
text (rep(10, 9), coef(model.ridge)[length(seq(0,10,0.1)),-1], colnames(train)[-9], pos=4, col=colors)
text(5.5, 0.4, adj=c(0,-1), "best lambda", col="black", cex=0.75)
## So we refit our final ridge regression model using the best lambda:
model.ridgereg.FINAL <- lm.ridge(lpsa ~ ., data=train, lambda = lambda.ridge)
(beta.ridgereg.FINAL <- coef(model.ridgereg.FINAL))
library(glmnet)
t <- as.numeric(train[,'lpsa'])
x <- as.matrix(train[,1:8])
model.lasso <- cv.glmnet (x, t, nfolds = 10)
plot(model.lasso)
# The '.' correspond to removed variables
coef(model.lasso)
# lambda.min is the value of lambda that gives minimum mean cross-validated error
model.lasso$lambda.min
# Predictions can be made based on the fitted cv.glmnet object; for instance, this would be the TRAINING error with the "optimal" lambda as chosen by LOOCV
predict (model.lasso, newx = x, s = "lambda.min")
library(glmnet)
install.packages("glmnet")
library(glmnet)
library(glmnet)
t <- as.numeric(train[,'lpsa'])
x <- as.matrix(train[,1:8])
model.lasso <- cv.glmnet (x, t, nfolds = 10)
plot(model.lasso)
# The '.' correspond to removed variables
coef(model.lasso)
# lambda.min is the value of lambda that gives minimum mean cross-validated error
model.lasso$lambda.min
# Predictions can be made based on the fitted cv.glmnet object; for instance, this would be the TRAINING error with the "optimal" lambda as chosen by LOOCV
predict (model.lasso, newx = x, s = "lambda.min")
library(caret)
## specify 10x10 CV
K <- 10; TIMES <- 10
trc <- trainControl (method="repeatedcv", number=K, repeats=TIMES)
install.packages("caret")
library(caret)
library(caret)
## specify 10x10 CV
K <- 10; TIMES <- 10
trc <- trainControl (method="repeatedcv", number=K, repeats=TIMES)
# STANDARD REGRESSION
model.std.10x10CV <- train (lpsa ~ ., data = train, trControl=trc, method='lm')
normalization.train <- (length(train$lpsa)-1)*var(train$lpsa)
NMSE.std.train <- crossprod(predict (model.std.10x10CV) - train$lpsa) / normalization.train
data <- train # by just in case
res <- replicate (TIMES, {
# shuffle the data
data <- data[sample(nrow(data)),]
# Create K equally sized folds
folds <- cut (1:nrow(data), breaks=K, labels=FALSE)
sse <- 0
# Perform 10 fold cross validation
for (i in 1:K)
{
valid.indexes <- which (folds==i,arr.ind=TRUE)
valid.data <- data[valid.indexes, ]
train.data <- data[-valid.indexes, ]
model.ridgereg <- lm.ridge (lpsa ~ ., data=train.data, lambda = lambda.ridge)
beta.ridgereg <- coef (model.ridgereg)
sse <- sse + crossprod (train[valid.indexes,'lpsa'] - beta.ridgereg[1]
- as.matrix(valid.data[,1:8]) %*% beta.ridgereg[2:9])
}
sse/K
})
data <- train # by just in case
res <- replicate (TIMES, {
# shuffle the data
data <- data[sample(nrow(data)),]
# Create K equally sized folds
folds <- cut (1:nrow(data), breaks=K, labels=FALSE)
sse <- 0
# Perform 10 fold cross validation
for (i in 1:K)
{
valid.indexes <- which (folds==i,arr.ind=TRUE)
valid.data <- data[valid.indexes, ]
train.data <- data[-valid.indexes, ]
model.ridgereg <- lm.ridge (lpsa ~ ., data=train.data, lambda = lambda.ridge)
beta.ridgereg <- coef (model.ridgereg)
sse <- sse + crossprod (train[valid.indexes,'lpsa'] - beta.ridgereg[1]
- as.matrix(valid.data[,1:8]) %*% beta.ridgereg[2:9])
}
sse/K
})
NMSE.ridge.train <- mean(res) / normalization.train
# LASSO REGRESSION
library(elasticnet)
install.packages("elasticnet")
# LASSO REGRESSION
library(elasticnet)
# LASSO REGRESSION
library(elasticnet)
model.lasso.10x10CV <- train (lpsa ~ ., data = train, trControl=trc, method='glmnet')
NMSE.lasso.train <- crossprod(predict (model.lasso.10x10CV) - train$lpsa) / normalization.train
# Choose the best model as the one with the lowest CV error:
NMSE.std.train; NMSE.ridge.train; NMSE.lasso.train
## This is the test NMSE:
normalization.test <- (length(test$lpsa)-1)*var(test$lpsa)
sse <- crossprod (test$lpsa - beta.ridgereg.FINAL[1]
- as.matrix(test[,1:8]) %*% beta.ridgereg.FINAL[2:9])
(NMSE.ridge <- sse/normalization.test)
# 1) In this lab you can see clearly how the volume of work increases when one
# 2) The final model -ridge regression- is not bad but leaves margin for improvement; the difference
# 2) The final model -ridge regression- is not bad but leaves margin for improvement; the difference
# between train (CV) and test errors is large. This is mainly due to the fact that
